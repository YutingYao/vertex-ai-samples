{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24f03e63",
   "metadata": {},
   "source": [
    "# Custom Training Job with Tensorboard Monitoring\n",
    "\n",
    "This notebook demonstrates how to submit a custom Vertex training job and monitor it using Vertex TensorBoard. \n",
    "\n",
    "## Scenario\n",
    "\n",
    "The training scenario is fine-tuning BERT on the [GLUE COLA](https://nyu-mll.github.io/CoLA/) dataset. \n",
    "\n",
    "## Notes\n",
    "\n",
    "- The training regimen utilizes  [TensorFlow NLP Modelling Toolkit](https://github.com/tensorflow/models/tree/master/official/nlp)\n",
    "- Due to a complexity of the ML task, the custom training job is configured to use a multi-gpu training node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cbc85e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as text  # A dependency of the preprocessing model\n",
    "import tensorflow_addons as tfa\n",
    "from official.nlp import optimization\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42a2db9",
   "metadata": {},
   "source": [
    "## Evironment setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "357357f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'jk-mlops-dev'\n",
    "REGION = 'us-west1'\n",
    "STAGING_BUCKET = 'gs://jk-vertex-staging' \n",
    "BASE_IMAGE = 'gcr.io/deeplearning-platform-release/tf2-gpu.2-5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec55de5",
   "metadata": {},
   "source": [
    "## Configure data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f717bb7",
   "metadata": {},
   "source": [
    "### Make BERT data preprocessing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c18c02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TFHUB_HANDLE_PREPROCESS = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
    "\n",
    "def make_bert_preprocess_model(sentence_features, seq_length=128):\n",
    "    \"\"\"Returns a model mapping string features to BERT inputs.\"\"\"\n",
    "\n",
    "    input_segments = [\n",
    "        tf.keras.layers.Input(shape=(), dtype=tf.string, name=ft)\n",
    "        for ft in sentence_features]\n",
    "\n",
    "    # Tokenize the text to word pieces.\n",
    "    bert_preprocess = hub.load(TFHUB_HANDLE_PREPROCESS)\n",
    "    tokenizer = hub.KerasLayer(bert_preprocess.tokenize, name='tokenizer')\n",
    "    segments = [tokenizer(s) for s in input_segments]\n",
    "\n",
    "    # Pack inputs. The details (start/end token ids, dict of output tensors)\n",
    "    # are model-dependent, so this gets loaded from the SavedModel.\n",
    "    packer = hub.KerasLayer(bert_preprocess.bert_pack_inputs,\n",
    "                            arguments=dict(seq_length=seq_length),\n",
    "                            name='packer')\n",
    "    model_inputs = packer(segments)\n",
    "    return tf.keras.Model(input_segments, model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c60d8b7",
   "metadata": {},
   "source": [
    "### Try BERT data preprocessing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e0c5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preprocess_model = make_bert_preprocess_model(['sentence1', 'sentence2'])\n",
    "test_text = [np.array(['some random test sentence']), \n",
    "             np.array(['another random sentence'])]\n",
    "text_preprocessed = test_preprocess_model(test_text)\n",
    "\n",
    "print('Keys           : ', list(text_preprocessed.keys()))\n",
    "print('Shape Word Ids : ', text_preprocessed['input_word_ids'].shape)\n",
    "print('Word Ids       : ', text_preprocessed['input_word_ids'][0, :16])\n",
    "print('Shape Mask     : ', text_preprocessed['input_mask'].shape)\n",
    "print('Input Mask     : ', text_preprocessed['input_mask'][0, :16])\n",
    "print('Shape Type Ids : ', text_preprocessed['input_type_ids'].shape)\n",
    "print('Type Ids       : ', text_preprocessed['input_type_ids'][0, :16])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49e3189",
   "metadata": {},
   "source": [
    "### Visualize BERT data preprocessing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d820cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(test_preprocess_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619e6e4c",
   "metadata": {},
   "source": [
    "## Configure the text classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffee5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "TFHUB_HANDLE_ENCODER = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3'\n",
    "\n",
    "def build_classifier_model(num_classes, dropout_ratio):\n",
    "    \"\"\"Creates a text classification model based on BERT encoder.\"\"\"\n",
    "\n",
    "    inputs = dict(\n",
    "        input_word_ids=tf.keras.layers.Input(shape=(None,), dtype=tf.int32),\n",
    "        input_mask=tf.keras.layers.Input(shape=(None,), dtype=tf.int32),\n",
    "        input_type_ids=tf.keras.layers.Input(shape=(None,), dtype=tf.int32),\n",
    "    )\n",
    "\n",
    "    encoder = hub.KerasLayer(TFHUB_HANDLE_ENCODER, trainable=True, name='encoder')\n",
    "    net = encoder(inputs)['pooled_output']\n",
    "    net = tf.keras.layers.Dropout(rate=dropout_ratio)(net)\n",
    "    net = tf.keras.layers.Dense(num_classes, activation=None, name='classifier')(net)\n",
    "    return tf.keras.Model(inputs, net, name='prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366c3459",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_ratio = 0.1\n",
    "classes = 2\n",
    "\n",
    "classifier_model = build_classifier_model(classes, dropout_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ccc5ed",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ab7fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_raw_result = classifier_model(text_preprocessed)\n",
    "print(tf.sigmoid(bert_raw_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bad5011",
   "metadata": {},
   "source": [
    "### Visualize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d2e9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(classifier_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525872ba",
   "metadata": {},
   "source": [
    "## Configure `tf.data` pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a0c5d2",
   "metadata": {},
   "source": [
    "### Load the `glue/cola` dataset\n",
    "\n",
    "We will use  [TensorFlow Datasets](https://www.tensorflow.org/datasets). Since the `glue/cola` dataset is rather small we will load to memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7303c312",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfds_name = 'glue/cola' \n",
    "\n",
    "tfds_info = tfds.builder(tfds_name).info\n",
    "num_classes = tfds_info.features['label'].num_classes\n",
    "num_examples = tfds_info.splits.total_num_examples\n",
    "sentence_features = list(tfds_info.features.keys())\n",
    "available_splits = list(tfds_info.splits.keys())\n",
    "labels_names = tfds_info.features['label'].names\n",
    "\n",
    "print(f'Using {tfds_name} from TFDS')\n",
    "print(f'This dataset has {num_examples} examples')\n",
    "print(f'Number of classes: {num_classes}')\n",
    "print(f'Features {sentence_features}')\n",
    "print(f'Splits {available_splits}')\n",
    "print(f'Labels names {labels_names}')\n",
    "\n",
    "with tf.device('/job:localhost'):\n",
    "  # batch_size=-1 is a way to load the dataset into memory\n",
    "  in_memory_ds = tfds.load(tfds_name, batch_size=-1, shuffle_files=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5484061f",
   "metadata": {},
   "source": [
    "### Show some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e41392",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = tf.data.Dataset.from_tensor_slices(in_memory_ds['train'])\n",
    "\n",
    "for row in sample_dataset.take(2):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6c31bc",
   "metadata": {},
   "source": [
    "### Create data ingestion pipelines.\n",
    "\n",
    "We will be training on a multi-gpu node using the `MirroredStrategy` distribution strategy. When using the `MirroredStrategy` each batch of the input is divided equally among the replicas. Typically, you would want to increase your batch size as you add more accelerators, so as to make effective use of the extra computing power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f5666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6020ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_per_replica = 16\n",
    "global_batch_size = batch_size_per_replica * strategy.num_replicas_in_sync\n",
    "features = ['sentence']\n",
    "\n",
    "\n",
    "def get_data_pipeline(in_memory_ds, info, split, batch_size,\n",
    "                           bert_preprocess_model):\n",
    "    is_training = split.startswith('train')\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(in_memory_ds[split])\n",
    "    num_examples = info.splits[split].num_examples\n",
    "\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(num_examples)\n",
    "        dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(lambda ex: (bert_preprocess_model(ex), ex['label']))\n",
    "    dataset = dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset, num_examples\n",
    "\n",
    "\n",
    "bert_preprocess_model = make_bert_preprocess_model(features)\n",
    "\n",
    "train_dataset, train_data_size = get_data_pipeline(\n",
    "      in_memory_ds, tfds_info, 'train', global_batch_size, bert_preprocess_model)\n",
    "\n",
    "validation_dataset, validation_data_size = get_data_pipeline(\n",
    "      in_memory_ds, tfds_info, 'validation', global_batch_size, bert_preprocess_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb488560",
   "metadata": {},
   "source": [
    "## Configure and test training\n",
    "\n",
    "### Compile the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad18f77",
   "metadata": {},
   "source": [
    "Fine-tuning follows the optimizer set-up from BERT pre-training (as in [Classify text with BERT](https://www.tensorflow.org/text/tutorials/classify_text_with_bert)): It uses the AdamW optimizer with a linear decay of a notional initial learning rate, prefixed with a linear warm-up phase over the first 10% of training steps (`num_warmup_steps`). In line with the BERT paper, the initial learning rate is smaller for fine-tuning (best of 5e-5, 3e-5, 2e-5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac512c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "init_lr = 2e-5\n",
    "dropout_ratio = 0.1\n",
    "num_classes = 2\n",
    "# steps_per_epoch = train_data_size // global_batch_size\n",
    "steps_per_epoch = 50\n",
    "\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = num_train_steps // 10\n",
    "validation_steps = validation_data_size // global_batch_size\n",
    "\n",
    "with strategy.scope():\n",
    "    classifier_model = build_classifier_model(num_classes, dropout_ratio)\n",
    "    optimizer = optimization.create_optimizer(\n",
    "        init_lr=init_lr,\n",
    "        num_train_steps=num_train_steps,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        optimizer_type='adamw')\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metrics = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "        'accuracy', dtype=tf.float32)\n",
    "    \n",
    "classifier_model.compile(optimizer=optimizer, loss=loss, metrics=[metrics]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ca07e8",
   "metadata": {},
   "source": [
    "### Start a short training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959b5aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.fit(\n",
    "      x=train_dataset,\n",
    "      validation_data=validation_dataset,\n",
    "      steps_per_epoch=steps_per_epoch,\n",
    "      epochs=epochs,\n",
    "      validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8589f6a0",
   "metadata": {},
   "source": [
    "## Run Vertex custom training job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c619ce18",
   "metadata": {},
   "source": [
    "### Intialize Vertex SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be2bc780",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    staging_bucket=STAGING_BUCKET\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e942861d",
   "metadata": {},
   "source": [
    "### Create a training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9e6b0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'trainer'\n",
    "if tf.io.gfile.exists(folder):\n",
    "    tf.io.gfile.rmtree(folder)\n",
    "tf.io.gfile.mkdir(folder)\n",
    "file_path = os.path.join(folder, 'train.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e74b6889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing trainer/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {file_path}\n",
    "\n",
    "\n",
    "# Copyright 2021 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as text  # A dependency of the preprocessing model\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "\n",
    "from official.nlp import optimization\n",
    "\n",
    "TFHUB_HANDLE_ENCODER = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3'\n",
    "TFHUB_HANDLE_PREPROCESS = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
    "TFDS_NAME = 'glue/cola' \n",
    "NUM_CLASSES = 2\n",
    "SENTENCE_FEATURE = 'sentence'\n",
    "\n",
    "LOCAL_MODEL_DIR = '/tmp/saved_model'\n",
    "LOCAL_TB_DIR = '/tmp/logs'\n",
    "LOCAL_CHECKPOINT_DIR = '/tmp/checkpoints'\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_integer('epochs', 2, 'Nubmer of epochs')\n",
    "flags.DEFINE_integer('per_replica_batch_size', 16, 'Per replica batch size')\n",
    "flags.DEFINE_float('init_lr', 2e-5, 'Initial learning rate')\n",
    "flags.DEFINE_float('dropout_ratio', 0.1, 'Dropout ratio')\n",
    "\n",
    "\n",
    "def make_bert_preprocess_model(sentence_features, seq_length=128):\n",
    "    \"\"\"Returns a model mapping string features to BERT inputs.\"\"\"\n",
    "\n",
    "    input_segments = [\n",
    "        tf.keras.layers.Input(shape=(), dtype=tf.string, name=ft)\n",
    "        for ft in sentence_features]\n",
    "\n",
    "    # Tokenize the text to word pieces.\n",
    "    bert_preprocess = hub.load(TFHUB_HANDLE_PREPROCESS)\n",
    "    tokenizer = hub.KerasLayer(bert_preprocess.tokenize, name='tokenizer')\n",
    "    segments = [tokenizer(s) for s in input_segments]\n",
    "\n",
    "    # Pack inputs. The details (start/end token ids, dict of output tensors)\n",
    "    # are model-dependent, so this gets loaded from the SavedModel.\n",
    "    packer = hub.KerasLayer(bert_preprocess.bert_pack_inputs,\n",
    "                            arguments=dict(seq_length=seq_length),\n",
    "                            name='packer')\n",
    "    model_inputs = packer(segments)\n",
    "    return tf.keras.Model(input_segments, model_inputs)\n",
    "\n",
    "\n",
    "def build_classifier_model(num_classes, dropout_ratio):\n",
    "    \"\"\"Creates a text classification model based on BERT encoder.\"\"\"\n",
    "\n",
    "    inputs = dict(\n",
    "        input_word_ids=tf.keras.layers.Input(shape=(None,), dtype=tf.int32),\n",
    "        input_mask=tf.keras.layers.Input(shape=(None,), dtype=tf.int32),\n",
    "        input_type_ids=tf.keras.layers.Input(shape=(None,), dtype=tf.int32),\n",
    "    )\n",
    "\n",
    "    encoder = hub.KerasLayer(TFHUB_HANDLE_ENCODER, trainable=True, name='encoder')\n",
    "    net = encoder(inputs)['pooled_output']\n",
    "    net = tf.keras.layers.Dropout(rate=dropout_ratio)(net)\n",
    "    net = tf.keras.layers.Dense(num_classes, activation=None, name='classifier')(net)\n",
    "    return tf.keras.Model(inputs, net, name='prediction')\n",
    "\n",
    "\n",
    "def get_data_pipeline(in_memory_ds, info, split, \n",
    "                      batch_size,  bert_preprocess_model):\n",
    "    \"\"\"Creates a sentence preprocessing pipeline.\"\"\"\n",
    "    \n",
    "    is_training = split.startswith('train')\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(in_memory_ds[split])\n",
    "    num_examples = info.splits[split].num_examples\n",
    "\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(num_examples)\n",
    "        dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(lambda ex: (bert_preprocess_model(ex), ex['label']))\n",
    "    dataset = dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset, num_examples\n",
    "\n",
    "\n",
    "def set_job_dirs():\n",
    "    \"\"\"Sets job directories based on env variables set by Vertex AI.\"\"\"\n",
    "    \n",
    "    model_dir = os.getenv('AIP_MODEL_DIR', LOCAL_MODEL_DIR)\n",
    "    tb_dir = os.getenv('AIP_TENSORBOARD_LOG_DIR', LOCAL_TB_DIR)\n",
    "    checkpoint_dir = os.getenv('AIP_CHECKPOINT_DIR', LOCAL_CHECKPOINT_DIR)\n",
    "    \n",
    "    return model_dir, tb_dir, checkpoint_dir\n",
    "    \n",
    "\n",
    "def main(argv):\n",
    "    \"\"\"Starts a training run.\"\"\"\n",
    "    \n",
    "    del argv\n",
    "    logging.info('Setting up training.')\n",
    "    logging.info('   epochs: {}'.format(FLAGS.epochs))\n",
    "    logging.info('   per_replica_batch_size: {}'.format(FLAGS.per_replica_batch_size))\n",
    "    logging.info('   init_lr: {}'.format(FLAGS.init_lr))\n",
    "    logging.info('   dropout_ratio: {}'.format(FLAGS.dropout_ratio))\n",
    "\n",
    "    # Set distribution strategy\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    \n",
    "    global_batch_size = (strategy.num_replicas_in_sync *\n",
    "                         FLAGS.per_replica_batch_size)\n",
    "    \n",
    "    # Configure input data pipelines\n",
    "    tfds_info = tfds.builder(TFDS_NAME).info\n",
    "    num_classes = tfds_info.features['label'].num_classes\n",
    "    num_examples = tfds_info.splits.total_num_examples\n",
    "    available_splits = list(tfds_info.splits.keys())\n",
    "    labels_names = tfds_info.features['label'].names\n",
    "    \n",
    "    with tf.device('/job:localhost'):\n",
    "        in_memory_ds = tfds.load(TFDS_NAME, batch_size=-1, shuffle_files=True)\n",
    "        \n",
    "    bert_preprocess_model = make_bert_preprocess_model([SENTENCE_FEATURE])\n",
    "\n",
    "    train_dataset, train_data_size = get_data_pipeline(\n",
    "        in_memory_ds, tfds_info, 'train', global_batch_size, bert_preprocess_model)\n",
    "\n",
    "    validation_dataset, validation_data_size = get_data_pipeline(\n",
    "        in_memory_ds, tfds_info, 'validation', global_batch_size, bert_preprocess_model)\n",
    "    \n",
    "    # Configure the model\n",
    "    steps_per_epoch = train_data_size // global_batch_size\n",
    "    num_train_steps = steps_per_epoch * FLAGS.epochs\n",
    "    num_warmup_steps = num_train_steps // 10\n",
    "    validation_steps = validation_data_size // global_batch_size\n",
    "    \n",
    "    with strategy.scope():\n",
    "        classifier_model = build_classifier_model(NUM_CLASSES, FLAGS.dropout_ratio)\n",
    "        optimizer = optimization.create_optimizer(\n",
    "            init_lr=FLAGS.init_lr,\n",
    "            num_train_steps=num_train_steps,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            optimizer_type='adamw')\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        metrics = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "            'accuracy', dtype=tf.float32)\n",
    "    \n",
    "    classifier_model.compile(optimizer=optimizer, loss=loss, metrics=[metrics])\n",
    "    \n",
    "    model_dir, tb_dir, checkpoint_dir = set_job_dirs()\n",
    "\n",
    "    # Configure Keras callbacks\n",
    "    callbacks = [tf.keras.callbacks.experimental.BackupAndRestore(backup_dir=checkpoint_dir)]\n",
    "    callbacks.append(tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=tb_dir, update_freq='batch'))\n",
    "    \n",
    "    logging.info('Starting training ...')\n",
    "    classifier_model.fit(\n",
    "        x=train_dataset,\n",
    "        validation_data=validation_dataset,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=FLAGS.epochs,\n",
    "        validation_steps=validation_steps,\n",
    "        callbacks=callbacks)\n",
    "        \n",
    "    # Save trained model\n",
    "    logging.info('Training completed. Saving the trained model to: {}'.format(model_dir))\n",
    "    classifier_model.save(model_dir)  \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    logging.set_verbosity(logging.INFO)\n",
    "    app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d69c948",
   "metadata": {},
   "source": [
    "### Configure and submit Vertex job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "788cd9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.utils.source_utils:Training script copied to:\n",
      "gs://jk-vertex-staging/aiplatform-2021-06-01-20:21:22.438-aiplatform_custom_trainer_script-0.1.tar.gz.\n",
      "INFO:google.cloud.aiplatform.jobs:Creating CustomJob\n"
     ]
    },
    {
     "ename": "InvalidArgument",
     "evalue": "400 The image 'gcr.io/deeplearning-platform-release/tf2-gpu.2-5' is not supported. Please use an image offered by Vertex AI for python package training.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    922\u001b[0m                                       wait_for_ready, compression)\n\u001b[0;32m--> 923\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_end_unary_response_blocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    924\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 826\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_InactiveRpcError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INVALID_ARGUMENT\n\tdetails = \"The image 'gcr.io/deeplearning-platform-release/tf2-gpu.2-5' is not supported. Please use an image offered by Vertex AI for python package training.\"\n\tdebug_error_string = \"{\"created\":\"@1622578882.788954611\",\"description\":\"Error received from peer ipv4:74.125.142.95:443\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":1062,\"grpc_message\":\"The image 'gcr.io/deeplearning-platform-release/tf2-gpu.2-5' is not supported. Please use an image offered by Vertex AI for python package training.\",\"grpc_status\":3}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-8d31d6b9ab0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;31m# callbacks to call within the Future (in same Thread)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/jobs.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, service_account, network, timeout, restart_job_on_worker_restart, tensorboard, sync)\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[0mversion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"v1beta1\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtensorboard\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"v1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m         self._gca_resource = self.api_client.select_version(version).create_custom_job(\n\u001b[0;32m-> 1203\u001b[0;31m             \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_job\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gca_resource\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m         )\n\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform_v1/services/job_service/client.py\u001b[0m in \u001b[0;36mcreate_custom_job\u001b[0;34m(self, request, parent, custom_job, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrpc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;31m# Done; return the response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"metadata\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgument\u001b[0m: 400 The image 'gcr.io/deeplearning-platform-release/tf2-gpu.2-5' is not supported. Please use an image offered by Vertex AI for python package training."
     ]
    }
   ],
   "source": [
    "job_name = job_name = \"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "base_output_dir = f'{GCS_BUCKET}/jobs/{job_name}'\n",
    "requirements = ['tf-models-official==2.5.0', 'tensorflow-text==2.5.0']\n",
    "args = ['--epochs=3', '--per_replica_batch_size=16']\n",
    "machine_type = 'n1-standard-8',\n",
    "accelerator_type = 'nvidia-tesla-t4'\n",
    "accelerator_count = 2\n",
    "\n",
    "job = vertex_ai.CustomJob.from_local_script(\n",
    "    display_name=job_name,\n",
    "    script_path='trainer/train.py',\n",
    "    container_uri=BASE_IMAGE,\n",
    "    requirements=requirements,\n",
    "    args=args\n",
    ")\n",
    "\n",
    "job.run(sync=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3020cdd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job.resource_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79853cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m70",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m70"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
